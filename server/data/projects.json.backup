[
  {
    "id": 1,
    "title": "Exoplanet Detection with Deep Learning",
    "domain": "astrophysics",
    "description": "Developing convolutional neural networks to detect exoplanets from light curve data captured by space telescopes.",
    "fullDescription": "<p>This project focuses on creating a deep learning model that can automatically detect exoplanets from the light curve data collected by space telescopes like Kepler and TESS. The model uses convolutional neural networks (CNNs) to identify the characteristic dips in starlight that occur when planets transit in front of their host stars.</p><p>The project required extensive preprocessing of raw light curve data to account for various systematic errors and stellar variability. The model was trained on confirmed exoplanet transits and achieved 92% accuracy on the test dataset, significantly reducing the manual effort required in the planet detection pipeline.</p>",
    "technologies": ["Python", "TensorFlow", "Keras", "Astropy", "Pandas", "NumPy"],
    "results": "The model achieved 92% accuracy on test data, successfully identifying 15 previously undetected planetary candidates in Kepler archive data.",
    "githubUrl": "https://github.com/example/exoplanet-detection",
    "notebookUrl": "https://github.com/example/exoplanet-detection/blob/main/notebook.ipynb",
    "featured": 1,
    "status": "completed"
  },
  {
    "id": 2,
    "title": "Electron-Photon particle classification with ResNet-15",
    "domain": "astrophysics",
    "description": "Using ResNet-15 architecture to classify electron-photon particles based on their energy signatures and track properties.",
    "fullDescription": "<p>This project focuses on developing a deep learning model that can accurately classify elementary particles (electrons and photons) based on their energy signatures and track properties in high-energy physics experiments. Using a ResNet-15 architecture, the model processes complex detector data to differentiate between particle types with high precision.</p><p>The model was trained on simulated and real data from particle accelerator experiments, with careful attention to various detector effects and systematic uncertainties. Cross-validation techniques were employed to ensure robustness across different experimental conditions.</p>",
    "technologies": ["PyTorch", "ResNet", "High-Energy Physics", "TensorFlow", "CUDA", "Data Augmentation"],
    "results": "Achieved 96% classification accuracy, significantly improving particle identification in LHC experiments and enabling more precise cross-section measurements.",
    "githubUrl": "https://github.com/example/electron-photon-classification",
    "notebookUrl": "https://github.com/example/electron-photon-classification/blob/main/analysis.ipynb",
    "featured": 1,
    "status": "completed"
  },
  {
    "id": 3,
    "title": "Super-Resolution and reconstruction of LHC events using GAN's",
    "domain": "astrophysics",
    "description": "Applying Generative Adversarial Networks to enhance resolution and reconstruct detailed event structures from noisy and incomplete Large Hadron Collider data.",
    "fullDescription": "<p>This project explores the application of Generative Adversarial Networks (GANs) to enhance the resolution of detector readings and reconstruct complete event structures from the Large Hadron Collider. The approach allows for improved analysis of particle collision events even when the detector data is noisy or incomplete.</p><p>The GAN architecture consists of a generator network that learns to create high-resolution, complete event representations, and a discriminator network that distinguishes between real and generated events. This adversarial training process results in a model that can produce physically accurate reconstructions while enhancing detail beyond the original detector resolution.</p>",
    "technologies": ["GANs", "PyTorch", "CUDA", "ROOT", "Scientific Visualization", "Particle Physics"],
    "results": "Demonstrated 3x improvement in spatial resolution for jet reconstruction, enabling more precise measurements of particle properties and interactions in high-energy physics experiments.",
    "githubUrl": "https://github.com/example/lhc-super-resolution",
    "notebookUrl": "https://github.com/example/lhc-super-resolution/blob/main/gan_model.ipynb",
    "featured": 0,
    "status": "in-progress"
  },
  {
    "id": 4,
    "title": "Diffusion models for fast and accurate simulations of LHC experiment data",
    "domain": "astrophysics",
    "description": "Implementing diffusion probabilistic models to generate realistic simulations of particle physics experiments, significantly accelerating the simulation process.",
    "fullDescription": "<p>This project implements diffusion probabilistic models to create fast, accurate simulations of particle physics experiments at the Large Hadron Collider. Traditional Monte Carlo simulations are computationally expensive, and this approach offers orders of magnitude speedup while maintaining physical accuracy.</p><p>The diffusion model gradually converts noise into realistic detector signals through an iterative denoising process. It's trained on high-fidelity simulation data to capture the complex underlying physics processes. The approach incorporates physical constraints to ensure conservation laws are respected in the generated data.</p>",
    "technologies": ["Diffusion Models", "JAX", "Flax", "High-Performance Computing", "GEANT4", "Statistical Physics"],
    "results": "Achieved 500x speedup in simulation time compared to traditional Monte Carlo methods while maintaining 98% accuracy in key physics observables.",
    "githubUrl": "https://github.com/example/diffusion-lhc-sim",
    "notebookUrl": "https://github.com/example/diffusion-lhc-sim/blob/main/model.ipynb",
    "featured": 1,
    "status": "in-progress"
  },
  {
    "id": 5,
    "title": "CEBRA-Based Data Processing Pipeline for EEG Analysis",
    "domain": "biology",
    "description": "Applying unsupervised learning techniques to identify patterns in gene expression data from cancer patients to discover potential subtypes.",
    "fullDescription": "<p>This project explores the application of unsupervised learning methods to analyze gene expression data from cancer patients. The goal was to identify previously unknown cancer subtypes that might respond differently to treatments or have different prognoses.</p><p>Working with RNA-seq data from The Cancer Genome Atlas (TCGA), I applied dimensionality reduction techniques like t-SNE and UMAP to visualize the high-dimensional gene expression data. This was followed by clustering algorithms including hierarchical clustering, k-means, and DBSCAN to identify potential cancer subtypes.</p><p>The project also included survival analysis to determine whether the identified clusters corresponded to clinically relevant differences in patient outcomes. Additionally, pathway enrichment analysis was performed to understand the biological mechanisms that differentiate the clusters.</p>",
    "technologies": ["R", "Bioconductor", "scikit-learn", "UMAP", "Clustering", "Survival Analysis"],
    "results": "Identified three distinct molecular subtypes in breast cancer samples with significantly different survival rates and treatment responses.",
    "githubUrl": "https://github.com/example/cancer-expression-clustering",
    "notebookUrl": "https://github.com/example/cancer-expression-clustering/blob/main/analysis.ipynb",
    "featured": 1,
    "status": "completed"
  },
  {
    "id": 6,
    "title": "Protein Structure Prediction",
    "domain": "biology",
    "description": "Developing a deep learning model inspired by AlphaFold to predict 3D protein structures from amino acid sequences.",
    "fullDescription": "<p>Protein structure prediction is one of the most challenging problems in computational biology. This project implements a simplified version of the AlphaFold approach to predict the three-dimensional structure of proteins from their amino acid sequences.</p><p>I developed a multi-stage pipeline that includes sequence alignment, feature extraction, and a deep neural network architecture combining attention mechanisms and convolutional layers. The model was trained on the PDB database of known protein structures.</p><p>Special attention was given to evaluating the quality of predicted structures using metrics like RMSD (Root Mean Square Deviation) and TM-score (Template Modeling score), which measure the similarity between predicted and experimental structures.</p><p>The project also includes a visualization component that allows users to interactively explore the predicted protein structures.</p>",
    "technologies": ["Python", "PyTorch", "BioPython", "Attention Networks", "Molecular Visualization", "HPC"],
    "results": "Achieved an average RMSD of 4.2Ã… on test proteins, enabling accurate structure prediction for several previously unsolved proteins.",
    "githubUrl": "https://github.com/example/protein-structure-prediction",
    "notebookUrl": "https://github.com/example/protein-structure-prediction/blob/main/model.ipynb",
    "featured": 1,
    "status": "in-progress"
  },
  {
    "id": 7,
    "title": "Ancient Text Classification",
    "domain": "humanities",
    "description": "Using NLP and machine learning to classify ancient manuscripts by time period, region, and authorship based on linguistic features.",
    "fullDescription": "<p>This project applies natural language processing and machine learning techniques to classify ancient manuscripts based on various attributes including time period, geographical region, and authorship.</p><p>Working with a corpus of digitized ancient texts from different historical periods and regions, I extracted a wide range of linguistic features including vocabulary distributions, syntactic patterns, and stylometric markers. These features were then used to train machine learning models for classification tasks.</p><p>The project faced several challenges related to working with ancient texts, including incomplete documents, spelling variations, and evolving language use over time. I implemented several preprocessing steps to address these challenges, including text normalization, handling of abbreviations, and dealing with lacunae (gaps in texts).</p><p>The classification models were evaluated using cross-validation techniques, with special attention to the interpretability of results to ensure they would be useful for humanities scholars.</p>",
    "technologies": ["NLP", "BERT", "Historical Linguistics", "Python", "spaCy", "Digital Humanities"],
    "results": "Achieved 84% accuracy in dating ancient manuscripts, helping scholars resolve debates about the provenance of several contentious texts.",
    "githubUrl": "https://github.com/example/ancient-text-classification",
    "notebookUrl": "https://github.com/example/ancient-text-classification/blob/main/classification.ipynb",
    "featured": 1,
    "status": "completed"
  },
  {
    "id": 8,
    "title": "ARC-AGI Experiments",
    "domain": "kaggle",
    "description": "Exploring methods to solve the Abstraction and Reasoning Corpus challenge as a pathway to more general artificial intelligence.",
    "fullDescription": "<p>This project tackles the Abstraction and Reasoning Corpus (ARC) challenge, which tests AI systems on their ability to solve novel reasoning tasks with minimal examples. The challenge is designed to evaluate progress toward more general AI capabilities beyond specialized models.</p><p>I developed an approach that combines symbolic reasoning, program synthesis, and neural networks to identify underlying patterns and generate solutions. The system attempts to discover the rules that transform input grids to output grids by testing hypotheses about the transformations involved.</p><p>The work involved creating a domain-specific language for representing transformations, a search algorithm to explore the space of possible programs, and a neural guidance module to prioritize promising directions.</p>",
    "technologies": ["Symbolic AI", "Program Synthesis", "Reinforcement Learning", "PyTorch", "Graph Neural Networks", "Meta-Learning"],
    "results": "Successfully solved 43% of ARC tasks, representing a significant improvement over baseline approaches and providing insights into abstraction capabilities in AI systems.",
    "githubUrl": "https://github.com/example/arc-agi",
    "notebookUrl": "https://github.com/example/arc-agi/blob/main/solver.ipynb",
    "featured": 1,
    "status": "in-progress"
  },
  {
    "id": 9,
    "title": "Nexar Dashcam Accident Prediction",
    "domain": "kaggle",
    "description": "Developing a real-time system to predict potential traffic accidents from dashcam video feeds, providing early warnings to drivers.",
    "fullDescription": "<p>This project aims to predict potential traffic accidents before they occur by analyzing real-time dashcam video feeds. Using data from the Nexar dashcam challenge, I developed a multi-modal system that combines computer vision, time series analysis, and contextual information to identify dangerous traffic situations.</p><p>The model processes video frames through a 3D CNN architecture to capture temporal patterns in traffic flow and driver behavior. It also incorporates GPS, accelerometer data, and map information to understand the driving context. A recurrent neural network integrates these signals to provide a continuous risk assessment.</p><p>Special attention was given to class imbalance issues, as accidents are rare events compared to normal driving conditions. Techniques including focal loss, data augmentation, and hard negative mining were employed to improve model sensitivity.</p>",
    "technologies": ["Computer Vision", "3D CNNs", "LSTMs", "TensorFlow", "Multi-modal Learning", "Time Series Analysis"],
    "results": "Achieved 87% precision and 83% recall in identifying high-risk situations 3-5 seconds before potential accidents, providing critical time for driver intervention.",
    "githubUrl": "https://github.com/example/dashcam-prediction",
    "notebookUrl": "https://github.com/example/dashcam-prediction/blob/main/model.ipynb",
    "featured": 1,
    "status": "completed"
  },
  {
    "id": 10,
    "title": "Quantum Neural Networks for Classification",
    "domain": "quantum",
    "description": "Implementing quantum neural networks on IBM's quantum computers to solve classification problems with potential quantum advantage.",
    "fullDescription": "<p>This project explores the implementation of quantum neural networks (QNNs) on real quantum hardware provided by IBM Quantum. The goal was to investigate whether these quantum models could offer advantages for certain classification tasks compared to classical neural networks.</p><p>I designed a variational quantum circuit that serves as the core of the QNN, with trainable rotation gates and entangling operations. The model was applied to several benchmark classification datasets, with a focus on problems where quantum correlations might be beneficial.</p><p>The project addressed various challenges in quantum machine learning, including circuit depth limitations on current hardware, noise mitigation strategies, and efficient parameter optimization techniques. Hybrid quantum-classical training approaches were employed to make the best use of both computing paradigms.</p>",
    "technologies": ["Qiskit", "Quantum Computing", "Variational Quantum Circuits", "PennyLane", "PyTorch", "IBM Quantum"],
    "results": "Demonstrated comparable performance to classical models on small datasets while requiring fewer parameters, and identified specific problem structures where quantum approaches show promise for future advantage.",
    "githubUrl": "https://github.com/example/quantum-nn",
    "notebookUrl": "https://github.com/example/quantum-nn/blob/main/qnn_classifier.ipynb",
    "featured": 1,
    "status": "in-progress"
  },
  {
    "id": 11,
    "title": "Qubit Control Optimization with Reinforcement Learning",
    "domain": "quantum",
    "description": "Using deep reinforcement learning to optimize quantum control pulses for faster and more robust gate operations in superconducting qubits.",
    "fullDescription": "<p>This project applies deep reinforcement learning techniques to optimize control pulses for quantum operations on superconducting qubits. The goal is to achieve faster and more robust quantum gates, which is essential for practical quantum computing.</p><p>I developed a simulation environment that accurately models the physics of superconducting qubits, including decoherence effects and control constraints. A reinforcement learning agent learns to design control pulses that implement target quantum operations while minimizing errors and operation time.</p><p>The approach uses a proximal policy optimization (PPO) algorithm with a custom reward function that balances fidelity, speed, and robustness to parameter variations. The learned control strategies were validated against traditional optimization methods and tested for transferability to real quantum hardware.</p>",
    "technologies": ["Reinforcement Learning", "QuTiP", "TensorFlow", "Quantum Control Theory", "Superconducting Qubits", "JAX"],
    "results": "Reduced gate time by 40% while maintaining 99.9% fidelity compared to standard techniques, with improved robustness to frequency drift and control field fluctuations.",
    "githubUrl": "https://github.com/example/quantum-control-rl",
    "notebookUrl": "https://github.com/example/quantum-control-rl/blob/main/optimization.ipynb",
    "featured": 0,
    "status": "not-started"
  },
  {
    "id": 12,
    "title": "Crypto Market Prediction with Transformer Models",
    "domain": "finance",
    "description": "Using transformer-based deep learning to predict cryptocurrency price movements by incorporating market sentiment and on-chain metrics.",
    "fullDescription": "<p>This project applies transformer-based deep learning models to predict cryptocurrency price movements. Unlike traditional approaches that rely solely on historical price data, this model incorporates market sentiment from social media and on-chain metrics to capture the unique dynamics of crypto markets.</p><p>I developed a multi-modal architecture that processes price time series, text data from Twitter and Reddit, and blockchain metrics such as transaction volume and active addresses. The transformer encoder effectively captures long-range dependencies and complex interactions between these different data sources.</p><p>The model was trained on several years of data covering multiple market cycles, with careful attention to preventing look-ahead bias and maintaining realistic evaluation conditions. Various trading strategies were backtested using the model's predictions to assess real-world applicability.</p>",
    "technologies": ["Transformers", "PyTorch", "NLP", "Time Series", "Blockchain Analytics", "Sentiment Analysis"],
    "results": "Achieved 62% directional accuracy for daily price movements and 5.8% alpha when backtested against a buy-and-hold strategy after accounting for transaction costs.",
    "githubUrl": "https://github.com/example/crypto-prediction",
    "notebookUrl": "https://github.com/example/crypto-prediction/blob/main/model.ipynb",
    "featured": 1,
    "status": "not-started"
  }
]
